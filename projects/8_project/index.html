<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Depth-Aware Pixel2Mesh | Kabir Jolly</title> <meta name="author" content="Kabir Jolly"> <meta name="description" content="CS 231N Final Project"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B2&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://kabir-jolly.github.io/projects/8_project/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kabir </span>Jolly</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/experience/">experience</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/resume/">resume</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Depth-Aware Pixel2Mesh</h1> <p class="post-description">CS 231N Final Project</p> </header> <article> <p><a href="http://cs231n.stanford.edu/" rel="external nofollow noopener" target="_blank">CS 231 (Deep Learning for Computer Vision)</a> has probably been my favorite class at Stanford so far. It was incredibly well taught and also dove deeper into many of the topics I found to be especially interesting when doing my own ML/DL research throughout the years. Even as early as high school, I remember watching <a href="https://www.youtube.com/watch?v=NfnWJUyUJYU&amp;list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&amp;pp=iAQB" rel="external nofollow noopener" target="_blank">Andrej Karpathy’s CS 231N</a> lectures on YouTube, and it was actually one of the reasons that got me excited to apply for Stanford in the first place.</p> <p>The course consists of a wide variety of topics, but I wanted to more specifically share my final project, which I had the pleasure of working on with Julian Quevedo and Rohin Manvi. If you would like to read the original paper, you can find it <a href="/assets/pdf/231N_Final_Paper.pdf">here</a>.</p> <h4> Background </h4> <p>Current advances in the computer vision space have become increasingly accurate in object detection when given 2D inputs.</p> <ul> <li>Models like Mask R-CNN</li> <li>Instance and semantic segmentation However, the world around us lies in 3D, and there is still much work to be done moving forward in developing a computational understanding of 3D shapes and objects.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/231n/231n1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="overview" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Mesh R-CNN is one major advancement in this space</p> <ul> <li>Constructs topologically accurate 3D meshes given a 2D RGB image using voxel representations</li> <li>Translates these voxel representations into a mesh using a GNN-based approach.</li> </ul> <h4> Proposed Solution </h4> <p>Current systems lack a major component of object recognition that we as humans use to perceive the world around us - <i>depth</i>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/231n/231n2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Current Methods" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We aim to expand upon existing models by enhancing its performance through depth aware inputs and log potential changes in performance as a result. The depth images will be generated using the MiDaS depth estimation model.</p> <div class="row" style="text-align:center;"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/231n/231n3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="depth" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4> Methodology and Results </h4> <h5> Phase 1: Differentiable Rendering </h5> <p>In order to provide further supervision on our generated meshes, we considered augmenting the loss with differentiable rendering.</p> <ul> <li>Utilize a differentiable rasterizer to render a depth map of it the generated mesh</li> <li>Error between the rendered depth map and the “ground-truth” depth channel can be measured</li> </ul> <p>This would allow our model to take further advantage of the RGB-D images by creating a mesh that has the same depth characteristics as the input depth map. In order to meaningfully compare the rendered depth maps with the ones from the input images we need the following</p> <ul> <li>The loss must be scale-invariant.</li> <li>Rendered depth maps must be from the same camera positions as the input images.</li> </ul> <p>We discovered more difficulties and were unable to fully implement the scale-invariant depth loss.</p> <ul> <li>MiDaS hallucinates a ground plane beneath the ShapeNet renderings.</li> <li>The differentiable renderer simply marks all background points as −1</li> <li>MiDaS outputs an inverse depth map We hope to investigate solving both these problems simultanoeously in future work.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/231n/231n4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="meshes" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h5> RGB-D Backbone and Mesh Refinement Head </h5> <p>To allow Mesh R-CNN to take RGB-D images as input, we changed the first ResNet layer to learn four-channel filters instead of three- channel filters. We take advantage of pretraining by copying over the weights of for the first three channels and only train the fourth from scratch.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/231n/231n5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="architecture" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Chamfer distance and the normal distance are used as losses for the mesh. Pointclouds P and Q are sampled from the ground truth and the intermediate mesh predictions from the model.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/231n/231n6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="losses" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4> Dataset and Features </h4> <p>We trained our model on two datasets: ShapeNet Core (along with renderings from R2N2) and Pix3D. ShapeNet Core consists of over 50,000 3D meshes, which R2N2 provides rendered images of.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/231n/231n7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="shapenet" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We use MiDaS to predict each image’s depth map which we stack to produce four-channel RGB-D images.</p> <h4> Results and Future Work </h4> <p>Adding depth resulted in a clear improvement for Pixel2Mesh, but seemed to make little difference for Mesh R-CNN.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/231n/231n8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="results-table" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>A possible extension is to construct colored meshes</p> <ul> <li>Accurately represent textures and materials that appear in images</li> <li>Since we represent the meshes as graphs, our idea is to incorporate color information as an additional node feature</li> <li>Thus, as supervision for our color predictions, we need the meshes in the datasets to have vertex colorings • To enable Mesh R-CNN to predict the color of each vertex, we plan to increase the dimension of the node features predicted by the mesh refinement stage</li> <li>Instead of predicting 3-dimensional features, we will predict 6-dimensional features, where the first three correspond to the vertex coordinate and the second three correspond to RGB values We also plan to continue the unfinished work on using differentiable rendering and the depth images during training. We hope to overcome the aforementioned roadblocks and hypothesize that due to the additional information fed in during training time, it is likely to outperform the results exhibited by the current depth-aware Pixel2Mesh.</li> </ul> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2023 Kabir Jolly. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: July 22, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>